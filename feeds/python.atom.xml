<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Joe Thomas - Python</title><link href="http://jsthomas.github.io/" rel="alternate"></link><link href="http://jsthomas.github.io/feeds/python.atom.xml" rel="self"></link><id>http://jsthomas.github.io/</id><updated>2013-12-10T00:00:00-05:00</updated><entry><title>A Camera-Based Virtual Keyboard System</title><link href="http://jsthomas.github.io/vkeyboard.html" rel="alternate"></link><published>2013-12-10T00:00:00-05:00</published><updated>2013-12-10T00:00:00-05:00</updated><author><name>Joe Thomas</name></author><id>tag:jsthomas.github.io,2013-12-10:/vkeyboard.html</id><summary type="html">&lt;p&gt;A brief summary of my term project for ECE 532&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the Fall of 2013, I developed a camera-based keyboard system as my
term project for an image analysis course. This page provides a brief
summary of my project.&lt;/p&gt;
&lt;p&gt;In my system, a webcam looks down on a user's hands, which rest on a
paper keyboard template. My program collects images from the webcam to
determine when and where the user touches the keyboard template. When
the user touches a key, my program produces the corresponding letter
as output.&lt;/p&gt;
&lt;p&gt;Virtual keyboards are still actively researched in computer science
and electrical engineering. The system I implemented uses techniques
published in [1]-[3] between 2010 and 2013. Camera-based virtual
keyboards require a number of interesting image analysis techniques to
implement. They are particularly of interest in mobile applications
and in areas where one would like to avoid the cost of a physical
keyboard (for example, projects providing computers to people in the
developing world).&lt;/p&gt;
&lt;h1&gt;Processing Steps&lt;/h1&gt;
&lt;p&gt;The virtual keyboard system I implemented needed to solve three main
problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Detect the locations of the user's fingertips.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Detect whether any fingertips are in contact with the tabletop.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When a fingertip touches the tabletop, determine which key was
pressed on the keyboard.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first problem can be solved by segmenting an input image &lt;span class="math"&gt;\(I\)&lt;/span&gt; into
two regions: one representing the hands and another representing the
background. For each connected component of the hand region, we can
extract a contour &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; that traverses the boundary
counterclockwise. Places where &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; has high curvature (the
places where &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; bends the most sharply) typically correspond to
fingertips.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Virtual keyboard processing steps." src="http://jsthomas.github.io/images/vkeyboard/kb_processing.png" style="width: 1680px; height: auto;"/&gt;&lt;/p&gt;
&lt;p&gt;I addressed the second problem using a technique called &lt;em&gt;shadow
analysis&lt;/em&gt;, which is discussed in [1]. With this technique, we perform
another segmentation on &lt;span class="math"&gt;\(I\)&lt;/span&gt;, to locate the regions in the image that
represent the shadows of the user's hands. Then, we examine small
neighborhoods near the user's fingertips. If the proportion of
shadow-pixels in a given neighborhood is above a certain threshold, we
infer that the corresponding fingertip is not in contact with the
tabletop (because it has a shadow). Otherwise, we declare that
fingertip to be touching the keyboard.&lt;/p&gt;
&lt;p&gt;The last problem is solved by making some assumptions about the
geometry of the keyboard and imaging system. I assumed that the
keyboard template displays four easily identifiable control points
arranged in a rectangle, and that the front edge of the control
rectangle is parallel to the &lt;span class="math"&gt;\(x\)&lt;/span&gt;-axis of the camera's coordinate
system. With these additional assumptions, it isn't too hard to invert
the perspective projection and recover the keyboard-space coordinates
of each keypress from the image-space coordinates of the fingertips.&lt;/p&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;p&gt;After implementing my virtual keyboard system in python, I analyzed
its performance under different lighting and usage conditions. My
tests suggest that a camera-based user interface that uses shadow
analysis will work best under conditions where one has fine control
over the lighting conditions and the user does not need to enter very
much data at one time. For example, a camera-based user interface
would work well in an interactive museum exhibit, but it might not be
a very good tool for word processing in a dark coffee shop. Currently,
camera-based keyboards cannot provide the speed and accuracy
achievable on physical keyboards (but for that matter, neither do
touchscreen keyboards).&lt;/p&gt;
&lt;h1&gt;Downloads&lt;/h1&gt;
&lt;p&gt;Below are links to the paper and code I wrote for the term project. I
used the code within &lt;a href="http://ipython.org/"&gt;ipython&lt;/a&gt; to conduct my
experiments, and used the &lt;a href="http://opencv.org/"&gt;OpenCV&lt;/a&gt; project to get
data from the camera. OpenCV's support for python is still under
development, so if you examine my code you may have to adapt it to
work with your particular hardware and copy of OpenCV.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Paper:
  &lt;a href="http://jsthomas.github.io/docs/vkeyboard/vkeyboard.pdf"&gt;A Camera Based Virtual Keyboard with Touch Detection by Shadow Analysis&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Project Code: &lt;a href="http://jsthomas.github.io/docs/vkeyboard/vkeyboard.zip"&gt;vkeyboard.zip&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Y. Adajania, J. Gosalia, A. Kanade, H. Mehta, and N. Shekokar. Virtual
keyboard using shadow analysis. In Emerging Trends in Engineering and
Technology (ICETET), 2010 3rd International Conference on, page 163 to
165, 2010.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;E. Posner, N. Starzicki, and E. Katz. A single camera based floating virtual
keyboard with improved touch detection. In Electrical Electronics Engineers
in Israel (IEEEI), 2012 IEEE 27th Convention of, page 1 to 5, 2012.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;M. Hagara and J. Pucik. Fingertip detection for virtual keyboard
based on camera. In Radioelektronika (RADIOELEKTRONIKA), 2013 23rd
International Conference, page 356 to 360, 2013.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content></entry></feed>